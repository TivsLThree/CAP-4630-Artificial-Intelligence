{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4V1-pretrained_convnet_finetuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TivsLThree/CAP-4630-Artificial-Intelligence/blob/master/Homework/HW4/HW4V1_pretrained_convnet_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSrgMJ5vKFzQ",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning VGG16 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIW7bZaAK-Hf",
        "colab_type": "text"
      },
      "source": [
        "This is based on section 5.3 *Using a pretrained convnet* of the book *Deep learning with Python* by Francois Chollet. I have made several changes to the code. I use the data that is already provided by Google. I don't download the data from Kaggle as in the deep learning book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsBq__dVo2pj",
        "colab_type": "text"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g6ETAM9o54M",
        "colab_type": "text"
      },
      "source": [
        "Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.\n",
        "\n",
        "We will use here the convolutional base of the VGG16 model to extract the features. We will feed these features to a densely connected classifier with dropout. We will fine-tune some layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqIG8QHMLNOn",
        "colab_type": "text"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3I1jd7cKV6_",
        "colab_type": "text"
      },
      "source": [
        "Download the example data, a zip. of 2,000 JPG pictures of cats and dogs and extract it locally in ```/tmp```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWjprHEXJ5Qi",
        "colab_type": "code",
        "outputId": "ef8f9983-2aa2-4e9c-d830-b712dda8e015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-16 22:44:27--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.167.128, 2a00:1450:400c:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.167.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  49.3MB/s    in 1.3s    \n",
            "\n",
            "2020-04-16 22:44:29 (49.3 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvoHtdA-K6Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shZiOBnJMyy_",
        "colab_type": "text"
      },
      "source": [
        "Note that the data provided by Google does not have a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL8ikM89LlsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inyP8bcdXnn-",
        "colab_type": "text"
      },
      "source": [
        "## Build network with VGG16 convolution base and custom densely connected layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ48W5T9rHWu",
        "colab_type": "text"
      },
      "source": [
        "### Load the convolutional base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0FgtANCXm_H",
        "colab_type": "code",
        "outputId": "ded03e1d-a660-4db4-88d8-81df7a5b523a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.applications import ResNet50\n",
        "from keras.applications import Xception\n",
        "# Change the conv_base\n",
        "#conv_base = VGG16(\n",
        "#    weights='imagenet', \n",
        "#    include_top=False, \n",
        "#    input_shape=(150, 150, 3))\n",
        "#conv_base = ResNet50( # didn't like these results\n",
        "#    include_top = False, \n",
        "#    weights = 'imagenet', \n",
        "#    input_shape=(150,150,3))\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 3s 0us/step\n",
            "Model: \"xception\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 74, 74, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 74, 74, 32)   128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 74, 74, 32)   0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 72, 72, 64)   18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 72, 72, 64)   256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 72, 72, 64)   0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 72, 72, 128)  8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 72, 72, 128)  512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 72, 72, 128)  0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 72, 72, 128)  17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 72, 72, 128)  512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 36, 36, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 36, 36, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 36, 36, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 36, 36, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 36, 36, 128)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 36, 36, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 36, 36, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 36, 36, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 36, 36, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 36, 36, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 18, 18, 256)  32768       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 18, 18, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 18, 18, 256)  1024        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 18, 18, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 18, 18, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 18, 18, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 18, 18, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 18, 18, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 18, 18, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 18, 18, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 9, 9, 728)    186368      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 9, 9, 728)    0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 9, 9, 728)    2912        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 9, 9, 728)    0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 9, 9, 728)    0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 9, 9, 728)    0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 9, 9, 728)    0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 9, 9, 728)    0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 9, 9, 728)    0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 9, 9, 728)    0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 9, 9, 728)    0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 9, 9, 728)    0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 9, 9, 728)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 9, 9, 728)    0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 9, 9, 728)    0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 9, 9, 728)    0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 9, 9, 728)    0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 9, 9, 728)    0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 9, 9, 728)    0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 9, 9, 728)    0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 9, 9, 728)    0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 9, 9, 728)    0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 9, 9, 728)    0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 9, 9, 728)    536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 9, 9, 728)    2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 9, 9, 728)    0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 9, 9, 728)    0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 9, 9, 728)    0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 9, 9, 728)    0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 9, 9, 728)    0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 9, 9, 728)    0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 9, 9, 728)    0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 9, 9, 728)    0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 9, 9, 728)    0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 9, 9, 728)    536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 9, 9, 728)    2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 9, 9, 728)    0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 9, 9, 728)    0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 9, 9, 728)    536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 9, 9, 728)    2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 9, 9, 728)    0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 9, 9, 1024)   752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 9, 9, 1024)   4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 5, 5, 1024)   745472      add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 5, 5, 1024)   0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 5, 5, 1024)   4096        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 5, 5, 1024)   0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 5, 5, 1536)   1582080     add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 5, 5, 1536)   6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 5, 5, 1536)   0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 5, 5, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 5, 5, 2048)   8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 5, 5, 2048)   0           block14_sepconv2_bn[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 20,861,480\n",
            "Trainable params: 20,806,952\n",
            "Non-trainable params: 54,528\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6OBHOO9q1ou",
        "colab_type": "text"
      },
      "source": [
        "### Freeze the convolutional base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UleRo4Dpq6Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfx6PqhPrb7Q",
        "colab_type": "text"
      },
      "source": [
        "### Concatenate the convolutional base and densely connected layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUpmocDAO3xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "# Make small changes to classifier. Reduced dense layer size and added the drop out.\n",
        "# I assume this dense layer reduction was way too much. May actually expand it\n",
        "# For the next classifier, I am considering adding another dropout between the flatten and dense node.\n",
        "# I may also add another dense layer with another activation to see if I get better results. \n",
        "# It seems like these changes may have made it worse. Must have reduced the dense layer too much. \n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6gZSeAjF7c",
        "colab_type": "code",
        "outputId": "52abd453-3026-4e8b-c853-53023c7e12da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "xception (Model)             (None, 5, 5, 2048)        20861480  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 51200)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               6553728   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 27,415,337\n",
            "Trainable params: 6,553,857\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZjBiddhi5Qj",
        "colab_type": "text"
      },
      "source": [
        "## Train the model end to end with frozen convolutional base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfAQlC2Oi41L",
        "colab_type": "code",
        "outputId": "135def99-3e12-47a4-b734-071cb66389bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "# data augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "# compile model\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',  # smaller learning rate\n",
        "    optimizer=optimizers.RMSprop(lr=1e-6), \n",
        "    metrics=['acc'])\n",
        "\n",
        "# train\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/30\n",
            "100/100 [==============================] - 30s 304ms/step - loss: 0.6745 - acc: 0.5985 - val_loss: 0.3186 - val_acc: 0.8650\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 21s 206ms/step - loss: 0.5830 - acc: 0.7015 - val_loss: 0.2392 - val_acc: 0.9210\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 21s 205ms/step - loss: 0.5285 - acc: 0.7415 - val_loss: 0.1441 - val_acc: 0.9370\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 20s 202ms/step - loss: 0.4987 - acc: 0.7635 - val_loss: 0.1993 - val_acc: 0.9500\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 21s 208ms/step - loss: 0.4709 - acc: 0.7915 - val_loss: 0.0460 - val_acc: 0.9580\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 20s 204ms/step - loss: 0.4419 - acc: 0.7945 - val_loss: 0.2557 - val_acc: 0.9540\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 20s 205ms/step - loss: 0.4259 - acc: 0.8090 - val_loss: 0.0387 - val_acc: 0.9570\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 21s 208ms/step - loss: 0.4068 - acc: 0.8170 - val_loss: 0.0414 - val_acc: 0.9560\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 20s 204ms/step - loss: 0.4085 - acc: 0.8125 - val_loss: 0.2068 - val_acc: 0.9620\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 21s 206ms/step - loss: 0.3931 - acc: 0.8230 - val_loss: 0.0436 - val_acc: 0.9590\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 20s 204ms/step - loss: 0.3880 - acc: 0.8255 - val_loss: 0.0156 - val_acc: 0.9590\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 20s 204ms/step - loss: 0.3859 - acc: 0.8215 - val_loss: 0.1507 - val_acc: 0.9620\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 20s 205ms/step - loss: 0.3601 - acc: 0.8380 - val_loss: 0.0504 - val_acc: 0.9630\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 20s 205ms/step - loss: 0.3620 - acc: 0.8325 - val_loss: 0.0085 - val_acc: 0.9620\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 21s 206ms/step - loss: 0.3664 - acc: 0.8265 - val_loss: 0.1231 - val_acc: 0.9620\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 21s 205ms/step - loss: 0.3734 - acc: 0.8235 - val_loss: 0.0769 - val_acc: 0.9640\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 21s 207ms/step - loss: 0.3538 - acc: 0.8400 - val_loss: 0.0360 - val_acc: 0.9630\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 20s 205ms/step - loss: 0.3449 - acc: 0.8400 - val_loss: 0.0605 - val_acc: 0.9650\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 21s 207ms/step - loss: 0.3514 - acc: 0.8400 - val_loss: 0.0356 - val_acc: 0.9620\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 21s 205ms/step - loss: 0.3567 - acc: 0.8410 - val_loss: 0.0138 - val_acc: 0.9650\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 21s 205ms/step - loss: 0.3509 - acc: 0.8275 - val_loss: 0.1418 - val_acc: 0.9650\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 21s 208ms/step - loss: 0.3388 - acc: 0.8515 - val_loss: 0.0993 - val_acc: 0.9630\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 21s 207ms/step - loss: 0.3477 - acc: 0.8335 - val_loss: 0.0183 - val_acc: 0.9650\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 21s 206ms/step - loss: 0.3237 - acc: 0.8530 - val_loss: 0.0034 - val_acc: 0.9680\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 21s 206ms/step - loss: 0.3482 - acc: 0.8315 - val_loss: 0.0201 - val_acc: 0.9630\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 20s 205ms/step - loss: 0.3553 - acc: 0.8400 - val_loss: 0.0072 - val_acc: 0.9640\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 21s 206ms/step - loss: 0.3325 - acc: 0.8495 - val_loss: 0.0225 - val_acc: 0.9660\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 22s 217ms/step - loss: 0.3497 - acc: 0.8325 - val_loss: 0.1048 - val_acc: 0.9640\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 22s 218ms/step - loss: 0.3301 - acc: 0.8480 - val_loss: 0.1440 - val_acc: 0.9650\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 22s 216ms/step - loss: 0.3230 - acc: 0.8580 - val_loss: 0.0411 - val_acc: 0.9680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqRc_vHKc92U",
        "colab_type": "text"
      },
      "source": [
        "## Display curves of loss and accuracy during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybWwdzz9bwuQ",
        "colab_type": "code",
        "outputId": "3ec98b10-3a8c-416f-ecd8-512309d08696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# training and validation accuracy\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='validation acc')\n",
        "plt.title('training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# training and validation loss\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
        "plt.title('training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOWsxpey8lN0",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwkVMx8N8qMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block1_conv1' or layer.name == 'block1_conv2':\n",
        "    layer.trainable = True;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyN-J7rs9cvo",
        "colab_type": "code",
        "outputId": "3ff162ff-38b4-44b1-bdc8-47f6973db5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compile model\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    #\n",
        "    # choose a smaller learning rate\n",
        "    #\n",
        "    optimizer=optimizers.RMSprop(lr=1e-6), \n",
        "    metrics=['acc'])\n",
        "\n",
        "# train\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 38s 384ms/step - loss: 0.1773 - acc: 0.9295 - val_loss: 0.1987 - val_acc: 0.9480\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1952 - acc: 0.9205 - val_loss: 0.1031 - val_acc: 0.9540\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1676 - acc: 0.9240 - val_loss: 0.0901 - val_acc: 0.9490\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1737 - acc: 0.9295 - val_loss: 0.3367 - val_acc: 0.9520\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1639 - acc: 0.9310 - val_loss: 0.1036 - val_acc: 0.9540\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1803 - acc: 0.9190 - val_loss: 0.3249 - val_acc: 0.9500\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1742 - acc: 0.9340 - val_loss: 0.1496 - val_acc: 0.9480\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1909 - acc: 0.9235 - val_loss: 0.0883 - val_acc: 0.9510\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1575 - acc: 0.9340 - val_loss: 0.0675 - val_acc: 0.9550\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1642 - acc: 0.9345 - val_loss: 0.0931 - val_acc: 0.9540\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.1587 - acc: 0.9420 - val_loss: 0.1137 - val_acc: 0.9570\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.1763 - acc: 0.9265 - val_loss: 0.0259 - val_acc: 0.9550\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1565 - acc: 0.9440 - val_loss: 0.0367 - val_acc: 0.9560\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1758 - acc: 0.9310 - val_loss: 0.1039 - val_acc: 0.9600\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1667 - acc: 0.9350 - val_loss: 0.1186 - val_acc: 0.9560\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1465 - acc: 0.9360 - val_loss: 0.0713 - val_acc: 0.9540\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1579 - acc: 0.9340 - val_loss: 0.0382 - val_acc: 0.9590\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1737 - acc: 0.9280 - val_loss: 0.0694 - val_acc: 0.9590\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.1599 - acc: 0.9375 - val_loss: 0.2333 - val_acc: 0.9600\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1651 - acc: 0.9295 - val_loss: 0.3019 - val_acc: 0.9560\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1536 - acc: 0.9350 - val_loss: 0.3672 - val_acc: 0.9610\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1392 - acc: 0.9410 - val_loss: 0.0509 - val_acc: 0.9610\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1394 - acc: 0.9475 - val_loss: 0.0561 - val_acc: 0.9570\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1441 - acc: 0.9365 - val_loss: 0.0127 - val_acc: 0.9620\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1599 - acc: 0.9360 - val_loss: 0.2066 - val_acc: 0.9590\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1285 - acc: 0.9490 - val_loss: 0.0112 - val_acc: 0.9590\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.1406 - acc: 0.9400 - val_loss: 0.1608 - val_acc: 0.9600\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1458 - acc: 0.9380 - val_loss: 0.0323 - val_acc: 0.9590\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1401 - acc: 0.9475 - val_loss: 0.0127 - val_acc: 0.9580\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1261 - acc: 0.9540 - val_loss: 0.1820 - val_acc: 0.9610\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1427 - acc: 0.9430 - val_loss: 0.0316 - val_acc: 0.9620\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1434 - acc: 0.9390 - val_loss: 0.0138 - val_acc: 0.9610\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1353 - acc: 0.9440 - val_loss: 0.0083 - val_acc: 0.9610\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1358 - acc: 0.9390 - val_loss: 0.0559 - val_acc: 0.9630\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1339 - acc: 0.9480 - val_loss: 0.1036 - val_acc: 0.9610\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 26s 265ms/step - loss: 0.1217 - acc: 0.9545 - val_loss: 0.0191 - val_acc: 0.9620\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.1469 - acc: 0.9395 - val_loss: 0.0707 - val_acc: 0.9590\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1468 - acc: 0.9445 - val_loss: 0.0347 - val_acc: 0.9610\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1213 - acc: 0.9495 - val_loss: 0.0558 - val_acc: 0.9590\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1178 - acc: 0.9585 - val_loss: 0.0486 - val_acc: 0.9600\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - 27s 265ms/step - loss: 0.1220 - acc: 0.9520 - val_loss: 0.0740 - val_acc: 0.9580\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1477 - acc: 0.9435 - val_loss: 0.1717 - val_acc: 0.9610\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1175 - acc: 0.9530 - val_loss: 0.0704 - val_acc: 0.9630\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1056 - acc: 0.9565 - val_loss: 0.2741 - val_acc: 0.9630\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1243 - acc: 0.9490 - val_loss: 0.0523 - val_acc: 0.9650\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1351 - acc: 0.9420 - val_loss: 0.1934 - val_acc: 0.9620\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1178 - acc: 0.9505 - val_loss: 0.0635 - val_acc: 0.9610\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1292 - acc: 0.9510 - val_loss: 0.0500 - val_acc: 0.9610\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1374 - acc: 0.9465 - val_loss: 0.0116 - val_acc: 0.9640\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1479 - acc: 0.9395 - val_loss: 0.0553 - val_acc: 0.9660\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1347 - acc: 0.9470 - val_loss: 0.1894 - val_acc: 0.9670\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1253 - acc: 0.9500 - val_loss: 0.0218 - val_acc: 0.9610\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1083 - acc: 0.9525 - val_loss: 0.0104 - val_acc: 0.9620\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1283 - acc: 0.9500 - val_loss: 0.0580 - val_acc: 0.9640\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1377 - acc: 0.9450 - val_loss: 0.1221 - val_acc: 0.9650\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1179 - acc: 0.9520 - val_loss: 0.2513 - val_acc: 0.9650\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1058 - acc: 0.9595 - val_loss: 0.0120 - val_acc: 0.9620\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1266 - acc: 0.9525 - val_loss: 0.0404 - val_acc: 0.9670\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1265 - acc: 0.9510 - val_loss: 0.0104 - val_acc: 0.9660\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1101 - acc: 0.9560 - val_loss: 0.0536 - val_acc: 0.9650\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1314 - acc: 0.9545 - val_loss: 0.1087 - val_acc: 0.9680\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1299 - acc: 0.9470 - val_loss: 0.1842 - val_acc: 0.9640\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1146 - acc: 0.9560 - val_loss: 0.0232 - val_acc: 0.9650\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1069 - acc: 0.9530 - val_loss: 0.0357 - val_acc: 0.9680\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1470 - acc: 0.9490 - val_loss: 0.0226 - val_acc: 0.9670\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1125 - acc: 0.9545 - val_loss: 0.0418 - val_acc: 0.9690\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1300 - acc: 0.9505 - val_loss: 0.0671 - val_acc: 0.9670\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1098 - acc: 0.9575 - val_loss: 0.0657 - val_acc: 0.9680\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0940 - acc: 0.9610 - val_loss: 0.0305 - val_acc: 0.9670\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0973 - acc: 0.9620 - val_loss: 0.0239 - val_acc: 0.9680\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1046 - acc: 0.9580 - val_loss: 0.0061 - val_acc: 0.9650\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1118 - acc: 0.9570 - val_loss: 0.0203 - val_acc: 0.9680\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1099 - acc: 0.9565 - val_loss: 0.0600 - val_acc: 0.9660\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1126 - acc: 0.9570 - val_loss: 0.0963 - val_acc: 0.9680\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0988 - acc: 0.9610 - val_loss: 0.0194 - val_acc: 0.9680\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1099 - acc: 0.9615 - val_loss: 0.0459 - val_acc: 0.9660\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1267 - acc: 0.9515 - val_loss: 0.0872 - val_acc: 0.9670\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1002 - acc: 0.9605 - val_loss: 0.0215 - val_acc: 0.9680\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1092 - acc: 0.9585 - val_loss: 0.1008 - val_acc: 0.9670\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1005 - acc: 0.9635 - val_loss: 0.1154 - val_acc: 0.9700\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.0956 - acc: 0.9670 - val_loss: 0.0418 - val_acc: 0.9680\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1028 - acc: 0.9635 - val_loss: 0.0402 - val_acc: 0.9670\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - 27s 269ms/step - loss: 0.1031 - acc: 0.9580 - val_loss: 0.0570 - val_acc: 0.9690\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0952 - acc: 0.9635 - val_loss: 0.3933 - val_acc: 0.9690\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1096 - acc: 0.9645 - val_loss: 0.1899 - val_acc: 0.9660\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1206 - acc: 0.9560 - val_loss: 0.0768 - val_acc: 0.9660\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0947 - acc: 0.9615 - val_loss: 0.0051 - val_acc: 0.9660\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.1082 - acc: 0.9595 - val_loss: 0.0577 - val_acc: 0.9700\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1005 - acc: 0.9610 - val_loss: 0.1337 - val_acc: 0.9710\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1011 - acc: 0.9625 - val_loss: 0.0266 - val_acc: 0.9690\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1178 - acc: 0.9580 - val_loss: 0.0721 - val_acc: 0.9700\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1043 - acc: 0.9565 - val_loss: 0.0286 - val_acc: 0.9660\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1088 - acc: 0.9560 - val_loss: 0.3165 - val_acc: 0.9680\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1132 - acc: 0.9600 - val_loss: 0.0333 - val_acc: 0.9700\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1083 - acc: 0.9580 - val_loss: 0.0702 - val_acc: 0.9700\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 0.1081 - acc: 0.9600 - val_loss: 0.0078 - val_acc: 0.9710\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.0726 - acc: 0.9715 - val_loss: 0.2103 - val_acc: 0.9710\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - 27s 267ms/step - loss: 0.1008 - acc: 0.9585 - val_loss: 0.0754 - val_acc: 0.9730\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.0949 - acc: 0.9645 - val_loss: 0.0438 - val_acc: 0.9710\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.0716 - acc: 0.9720 - val_loss: 0.0590 - val_acc: 0.9690\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVBQgh5M-Rtz",
        "colab_type": "text"
      },
      "source": [
        "## Display learning curves during fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbkIw7Ie-NP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# training and validation accuracy\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='validation acc')\n",
        "plt.title('training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# training and validation loss\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
        "plt.title('training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obQi6N4ZShdi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he3Nw6TOJwjx",
        "colab_type": "text"
      },
      "source": [
        "## Print out validation loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVQtrk7uJdhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loss, val_acc = model.evaluate_generator(validation_generator, steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdddLi1SUMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Validation loss:\", val_loss)\n",
        "print(\"Validation accuracy:\", val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ww756WToby8",
        "colab_type": "text"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lTUpUdwobKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_fname = 'cats_and_dogs_small_4.h5' \n",
        "model.save(model_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhEoI8ZTok-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open(model_fname, 'r') as f:\n",
        "  files.download(model_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNEL4CCcS0Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}